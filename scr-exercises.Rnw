\documentclass{article}

\usepackage{amsmath}
\usepackage[natbibapa,nosectionbib]{apacite}
\usepackage{enumitem}
\usepackage[textwidth = 6in, top = 2cm, bottom = 2cm]{geometry}
\usepackage{parskip}
\usepackage{xparse}
\DeclareDocumentCommand{\sct}{ O{} O{} m }{\shortcites{#3}\citet[#1][#2]{#3}}

\newcommand{\answer}[1]{\textcolor{red}{#1}}

\title{Simple SCR exercises}
\date{}

\begin{document}

\maketitle

The code in the file \texttt{scr-ll.r} creates a log-likelihood
function, loads some example data, and fits the `binary proximity
model' described by \sct{Efford2009}. The model makes the following
assumptions:
\begin{itemize}
\item The number of animals' activity centres in the survey region is
  a Poisson random variable, with expectation equal to animal density,
  $D$, multiplied by the area of the survey region.
\item The activity centre locations are independent, and are uniformly
  distributed across the survey region.
\item The probability that a detector detects an individual is given
  by the halfnormal detection function,
  \begin{equation*}
    g(d) = g_0 \exp\left( \frac{-d^2}{2\sigma^2} \right),
  \end{equation*}
  where $d$ is the distance between the animal's activity centre and
  the detector.
\end{itemize}

Run the code in \texttt{scr-ll.r} and answer the following questions.

\begin{enumerate}

<<package-load, message = FALSE, echo = FALSE>>=
library(spatstat)
@ 
  
<<setup, echo = FALSE, message = FALSE, cache = TRUE>>=
source("scr-ll.r")
@ 
  
\section*{General questions}
  
\item Create a plot of the detector locations. Their coodinates can be
  found in \texttt{test.data\$traps}.
  
<<plot-detectors, fig.width = 3, out.width = "3in", fig.height = 3, out.height = "3in", tidy = FALSE>>=
par(mar = c(4, 4, 0, 0), oma = rep(1, 4))
plot(test.data$traps, asp = 1, pch = as.character(1:9), 
     xlab = "x-coordinate", ylab = "y-coordinate")
@   

\answer{We have a three-by-three grid of detectors with a spacing of
  100 m between them.}

\item Inspect the capture histories in
  \texttt{test.data\$bin.capt}. Describe what the first two rows
  represent.
  
<<head-capt>>=
head(test.data$bin.capt, 2)
@ 

\answer{The first animal was detected by detectors 1 and 5
  (bottom-left and middle). The second animal was detected by
  detectors 1, 2, 5, 8, and 9 (bottom-left, middle-left, middle,
  middle-right, and top-right).}

\item The model has estimated animal density, $D$, and parameters of a
  halfnormal detection function, $g_0$ and $\sigma$. Create a plot of
  the detection function estimated by the model.

<<plot-detfn, fig.width = 6, out.width = "6in", fig.height = 3.5, out.height = "3.5in">>=
par(mar = c(4, 4, 0, 0), oma = rep(0.1, 4), xaxs = "i")
xx <- seq(0, 650, length.out = 1000)
g0 <- plogis(fit$par[2])
sigma <- exp(fit$par[3])
yy <- g0*exp(-xx^2/(2*sigma^2))
plot(xx, yy, type = "l", xlab = "Distance", ylab = "Detection probability")
abline(h = 0, lty = "dotted")
@ 
  
\item Tricky question for STATS 730 graduates only:
  \begin{enumerate}
  \item[(a)] Compute standard errors for the transformed parameters,
    $\log(D)$, $\text{logit}(g_0)$, and $\log(\sigma)$. The
    \texttt{optim()} argument \texttt{hessian} or the
    \texttt{optimHess()} function will be useful.

<<link-ses, tidy = FALSE>>=
hess <- optimHess(fit$par, scr.nll, capt = test.data$bin.capt, 
                  traps = test.data$traps, mask = test.data$mask)
vcov.link <- solve(hess)
sqrt(diag(vcov.link))
@ 
    
  \item[(b)] Compute standard errors for the parameters themselves, $D$,
    $g_0$, and $\sigma$.
    
<<unlink-ses, tidy = FALSE>>=
jacobian <- diag(3)
diag(jacobian) <- c(exp(fit$par[1]), dlogis(fit$par[2]), exp(fit$par[3]))
vcov.unlink <- jacobian %*% vcov.link %*% t(jacobian)
sqrt(diag(vcov.unlink))
@ 
    
  \item[(c)] Compute confidence intervals for the three parameters.
 
    \answer{Calculating a confidence interval for the transformed
      parameters and then back-transforming.}
<<unlink-ci1>>=
ses.link <- sqrt(diag(vcov.link))
## For D:
exp(fit$par[1] + c(-1, 1)*qnorm(0.975)*ses.link[1])
## For g0:
plogis(fit$par[2] + c(-1, 1)*qnorm(0.975)*ses.link[2])
## For sigma:
exp(fit$par[3] + c(-1, 1)*qnorm(0.975)*ses.link[3])
@ 

\answer{Constructing the confidence intervals on the untransformed
  parameters directly is possible, but not recommended. We would
  expect the transformed versions to be better approximated by a
  normal distribution. We also get confidence intervals going
  out-of-bounds (negative $D < 0$ and $g_0 > 1$) if we use this
  approach.}
  
<<unlink-ci2>>=
ses.unlink <- sqrt(diag(vcov.unlink))
## For D:
exp(fit$par[1]) + c(-1, 1)*qnorm(0.975)*ses.unlink[1]
## For g0:
plogis(fit$par[2]) + c(-1, 1)*qnorm(0.975)*ses.unlink[2]
## For sigma:
exp(fit$par[3]) + c(-1, 1)*qnorm(0.975)*ses.unlink[3]
@ 

\answer{Bonus points if you computed profile-likelihood confidence
  intervals---that's probably the best option, but quite a lot more
  work!}

\end{enumerate}
  
\item Write some R code that simulates capture histories from a
  spatial capture-recapture model under the following conditions:
  \begin{itemize}
  \item The survey region is a square, with x-coordinate limits
    $(-500, 900)$ and y-coordinate limits also $(-500, 900)$. Note
    that these coordinates are given in metres.
  \item Detectors are deployed on a three-by-three grid with a $100$ m
    spacing between them, so that the columns are located at
    x-coordinates $100$, $200$, and $300$, and the rows are located at
    y-coordinates $100$, $200$, and $300$. Note that this is the
    configuration of the detectors in \texttt{test.data\$traps}.
  \item Animal density is $D = 0.75$ animals per hectare. Note that
    $1$ hectare is $10\,000$ m$^2$.
  \item Conditional on its activity centre location, an individual is
    detected by a detector with probability given by a halfnormal
    detection function with $g_0 = 0.9$ and $\sigma = 75$ m.
  \end{itemize}

  For bonus points, write your R code as a function, allowing the user
  to set their own detector locations and parameter values.

<<sim-fun, tidy = FALSE>>=
## Arguments:
##
## pars: A vector of parameters (D, g0, sigma).
## region.lims: A vector of survey region limits (x.lower, x.upper, y.lower, y.upper).
## traps: Detector locations as a matrix of coordinates.
sim.simplescr <- function(pars, region.lims, traps){
    ## Extracting parameter values.
    D <- pars[1]
    g0 <- pars[2]
    sigma <- pars[3]
    ## Calculating area of survey region in hectares.
    region.area <- (region.lims[2] - region.lims[1])*
        (region.lims[4] - region.lims[3])/10000
    ## Extracting number of detectors.
    n.traps <- nrow(traps)
    ## Simulating number of animals.
    n.acs <- rpois(1, D*region.area)
    ## Simulating activity centre locations.
    ac.locs.x <- runif(n.acs, region.lims[1], region.lims[2])
    ac.locs.y <- runif(n.acs, region.lims[3], region.lims[4])
    ac.locs <- cbind(ac.locs.x, ac.locs.y)
    ## Calculating a matrix of differences between activity centres and detectors.
    dists <- crossdist(ac.locs[, 1], ac.locs[, 2],
                       traps[, 1], traps[, 2])
    ## Calculating detection probabilities.
    det.probs <- g0*exp(-dists^2/(2*sigma^2))
    ## Creating capture histories.
    capt.full <- matrix(rbinom(n.acs*n.traps, 1, det.probs),
                        nrow = n.acs, ncol = n.traps)
    ## We only observe capture histories with at least one detection.
    capt <- capt.full[apply(capt.full, 1, sum) > 0, ]
    capt
}
set.seed(1234)
capt.sim <- sim.simplescr(c(0.75, 0.9, 75), c(-500, 900, -500, 900),
                          test.data$traps)
capt.sim
@  

\item Fit a spatial capture-recapture model to your simulated
  data. Note that you can use the detector locations in
  \texttt{test.data\$traps} and the mask in
  \texttt{test.data\$mask}. How close are your estimates to the true
  parameter values?

<<fit-sim, cache = TRUE, tidy = FALSE>>=
fit.sim <- optim(par.start, scr.nll, capt = capt.sim, traps = test.data$traps, 
                 mask = test.data$mask)
## For D:
exp(fit.sim$par[1])
## For g0:
plogis(fit.sim$par[2])
## For sigma:
exp(fit.sim$par[3])
@ 

\answer{All three are surprisingly close. In fact, closer than I'd
  expect---I think this is partly luck!}
  
\item For STATS 730 graduates only: Compute confidence intervals for
  the three parameters. Did they capture the true parameter values?
  
<<sim-unlink-ci>>=
hess.sim <- optimHess(fit.sim$par, scr.nll, capt = capt.sim, 
                      traps = test.data$traps, mask = test.data$mask)
vcov.link.sim <- solve(hess.sim)
ses.link.sim <- sqrt(diag(vcov.link.sim))
## For D:
exp(fit.sim$par[1] + c(-1, 1)*qnorm(0.975)*ses.link.sim[1])
## For g0:
plogis(fit.sim$par[2] + c(-1, 1)*qnorm(0.975)*ses.link.sim[2])
## For sigma:
exp(fit.sim$par[3] + c(-1, 1)*qnorm(0.975)*ses.link.sim[3])
@

\answer{Yes, all three parameters fall within their respective
  confidence intervals.}

\item Run a simulation study, repeating Questions 5--7 a total of 100
  times. This involves simulating 100 sets of capture histories, and
  generating estimates from each. Inspect your 100 sets of estimates.

<<simstudy, tidy = FALSE, eval = FALSE>>=
set.seed(4321)
n.sims <- 100
par.ests <- matrix(0, nrow = n.sims, ncol = 3)
D.cis <- matrix(0, nrow = n.sims, ncol = 2)
g0.cis <- matrix(0, nrow = n.sims, ncol = 2)
sigma.cis <- matrix(0, nrow = n.sims, ncol = 2)
for (i in 1:n.sims){
    cat(i, "\n")
    capt.sim <- sim.simplescr(c(0.75, 0.9, 75), c(-500, 900, -500, 900),
                              test.data$traps)
    fit.sim <- optim(par.start, scr.nll, capt = capt.sim, traps = test.data$traps, 
                     mask = test.data$mask)
    par.ests[i, ] <- c(exp(fit.sim$par[1]), plogis(fit.sim$par[2]),
                       exp(fit.sim$par[3]))
    hess.sim <- optimHess(fit.sim$par, scr.nll, capt = capt.sim, 
                          traps = test.data$traps, mask = test.data$mask)
    vcov.link.sim <- solve(hess.sim)
    ses.link.sim <- sqrt(diag(vcov.link.sim))
    ## For D:
    D.cis[i, ] <- exp(fit.sim$par[1] + c(-1, 1)*qnorm(0.975)*ses.link.sim[1])
    ## For g0:
    g0.cis[i, ] <- plogis(fit.sim$par[2] + c(-1, 1)*qnorm(0.975)*ses.link.sim[2])
    ## For sigma:
    sigma.cis[i, ] <- exp(fit.sim$par[3] + c(-1, 1)*qnorm(0.975)*ses.link.sim[3])
}
@   
  
  \begin{enumerate}
  \item[(a)] How close are the averages of your parameter estimates to
    the true parameter values?
  \item[(b)] For STATS 730 graduates only: How often do your confidence
    intervals capture the true parameter values?
  \end{enumerate}

<<simstudy-res, echo = -1>>=
load("sim-res.RData")
## Averages across all simulations. Note that the model fit on the first 
## iteration did not converge properly and gave me a silly answer, so I
## discarded it.
apply(par.ests[-1, ], 2, mean)
@ 

\answer{The averages for all three parameters are close to the true
  parameter values. Our estimators appear more-or-less unbiased. We'd
  probably get a better indication with a larger number of
  iterations.}

  \section*{Questions for \texttt{ascr} users}
  
\item Fit the same model from \texttt{secr-ll.r}, but using the
  \texttt{ascr} package. Verify that you get the same parameter
  estimates. For STATS 730 graduates, also verify that you get similar
  standard errors and confidence intervals.
  
<<fit-ascr, message = FALSE, tidy = FALSE>>=
library(ascr)
## Fitting the model.
fit.ascr <- fit.ascr(capt = list(bincapt = test.data$bin.capt),
                     traps = test.data$traps, mask = test.data$mask)
## Looking at the estimates and standard errors.
summary(fit.ascr)
## Comparison with our estimates and standard errors.
cbind(c(exp(fit$par[1]), plogis(fit$par[2]), exp(fit$par[3])),
      ses.unlink)
## Looking at the confidence intervals.
confint(fit.ascr, linked = TRUE)
## Comparison with our confidence intervals.
rbind(exp(fit$par[1] + c(-1, 1)*qnorm(0.975)*ses.link[1]),
      plogis(fit$par[2] + c(-1, 1)*qnorm(0.975)*ses.link[2]),
      exp(fit$par[3] + c(-1, 1)*qnorm(0.975)*ses.link[3]))
@ 

\answer{Our estimates, standard errors, and confidence intervals are
  all very similar. Note that, by default, \texttt{ascr} appears to
  construct the confidence intervals directly on the untransformed
  parameters, which isn't entirely sensible! You need to add the
  argument \texttt{linked = TRUE} to calculate confidence intervals
  for the transformed parameters, and then convert them to the
  untransformed parameters. I should probably change the default
  behaviour.}
  
  \section*{Questions for \texttt{secr} users}

\item Fit the same model from \texttt{secr-ll.r}, but using the
  \texttt{secr} package. Verify that you get the same parameter
  estimates. For STATS 730 graduates, also verify that you get similar
  standard errors and confidence intervals. This will require some
  data reformatting.

<<fit-secr, message = FALSE, warning = FALSE, tidy = FALSE>>=
library(secr)
## Reformatting the data.
capt.secr <- convert.capt.to.secr(list(bincapt = test.data$bin.capt),
                                  traps = test.data$traps)
mask.secr <- convert.mask(test.data$mask)
traps.secr <- convert.traps(test.data$traps)
## Fitting the model.
fit.secr <- secr.fit(capthist = capt.secr, mask = mask.secr, trace = FALSE)
## Looking at the estimates, standard errors, and confidence intervals.
predict(fit.secr)
## Comparison with our estimates and standard errors.
cbind(c(exp(fit$par[1]), plogis(fit$par[2]), exp(fit$par[3])),
      ses.unlink)
## Comparison with our confidence intervals.
rbind(exp(fit$par[1] + c(-1, 1)*qnorm(0.975)*ses.link[1]),
      plogis(fit$par[2] + c(-1, 1)*qnorm(0.975)*ses.link[2]),
      exp(fit$par[3] + c(-1, 1)*qnorm(0.975)*ses.link[3]))
@ 

\answer{The standard errors are a little different for some reason,
  but the estimates and confidence intervals are very similar.}

\end{enumerate}

\bibliographystyle{tcite}
\bibliography{$HOME/mega/bib/refs}%$

\end{document}
